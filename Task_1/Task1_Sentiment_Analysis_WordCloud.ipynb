{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "# for i in range(1, pages + 1):\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified | Although transferring to thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified |   We are extremely grateful ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |   I had an appalling experie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not Verified |  Good points, the cabin crew, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Not Verified |  It was a decent flight, reason...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  ✅ Trip Verified | Although transferring to thi...\n",
       "1  ✅ Trip Verified |   We are extremely grateful ...\n",
       "2  ✅ Trip Verified |   I had an appalling experie...\n",
       "3  Not Verified |  Good points, the cabin crew, t...\n",
       "4  Not Verified |  It was a decent flight, reason..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "df.to_csv(\"data/BA_reviews.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data cleaned and saved to 'data/BA_reviews_cleaned.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load your raw reviews dataset\n",
    "df = pd.read_csv('data/BA_reviews.csv')\n",
    "\n",
    "# Function to clean review text\n",
    "def clean_review(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    # Remove verification text and \"|\"\n",
    "    text = text.replace(\"✅ Trip Verified\", \"\")\n",
    "    text = text.replace(\"Not Verified\", \"\")\n",
    "    text = text.replace(\"|\", \"\")\n",
    "    # Replace multiple spaces and newlines with single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply cleaning to each row\n",
    "df['reviews'] = df['reviews'].apply(clean_review)\n",
    "\n",
    "# Drop any rows where reviews column is now empty\n",
    "df = df[df['reviews'].str.strip() != \"\"]\n",
    "\n",
    "# Save cleaned data\n",
    "df.to_csv('data/BA_reviews_cleaned.csv', index=False)\n",
    "\n",
    "print(\"✅ Data cleaned and saved to 'data/BA_reviews_cleaned.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\4g traders\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.2.3)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 6.9 MB/s eta 0:00:00\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp310-cp310-win_amd64.whl (299 kB)\n",
      "     -------------------------------------- 299.8/299.8 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "     ---------------------------------------- 8.1/8.1 MB 12.3 MB/s eta 0:00:00\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.19.0-py3-none-any.whl (624 kB)\n",
      "     ------------------------------------- 624.3/624.3 kB 13.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\4g traders\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\4g traders\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\4g traders\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\4g traders\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "     ------------------------------------- 307.7/307.7 kB 18.6 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "     ------------------------------------- 274.0/274.0 kB 16.5 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.5/78.5 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click in c:\\users\\4g traders\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from nltk) (8.2.0)\n",
      "Collecting pillow\n",
      "  Downloading pillow-11.2.1-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 2.7/2.7 MB 12.2 MB/s eta 0:00:00\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "     ---------------------------------------- 111.1/111.1 kB ? eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\4g traders\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib) (25.0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "     ---------------------------------------- 71.9/71.9 kB ? eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.58.0-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 12.9 MB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "     ------------------------------------- 221.2/221.2 kB 13.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\4g traders\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\4g traders\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, pyparsing, pillow, kiwisolver, joblib, fonttools, cycler, contourpy, nltk, matplotlib, wordcloud, textblob\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.0 joblib-1.5.0 kiwisolver-1.4.8 matplotlib-3.10.3 nltk-3.9.1 pillow-11.2.1 pyparsing-3.2.3 regex-2024.11.6 textblob-0.19.0 tqdm-4.67.1 wordcloud-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas nltk wordcloud matplotlib textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "\n",
    "# Download NLTK data if you haven't already\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your CSV file (update the path accordingly)\n",
    "df = pd.read_csv('data/BA_reviews.csv')\n",
    "\n",
    "# Assuming the reviews column is named 'reviews' (adjust if needed)\n",
    "reviews = df['reviews'].astype(str)\n",
    "\n",
    "# Clean text: remove '✅ Trip Verified |' and 'Not Verified |' etc.\n",
    "reviews_clean = reviews.str.replace(r'✅ Trip Verified \\|', '', regex=True)\n",
    "reviews_clean = reviews_clean.str.replace(r'Not Verified \\|', '', regex=True)\n",
    "reviews_clean = reviews_clean.str.strip()\n",
    "\n",
    "# Sentiment Analysis function\n",
    "def get_sentiment(text):\n",
    "    analysis = TextBlob(text)\n",
    "    return analysis.sentiment.polarity  # Returns a float within [-1.0, 1.0]\n",
    "\n",
    "# Apply sentiment analysis to all reviews\n",
    "df['sentiment'] = reviews_clean.apply(get_sentiment)\n",
    "\n",
    "# Print average sentiment score\n",
    "print(f\"Average Sentiment Polarity: {df['sentiment'].mean():.3f}\")\n",
    "\n",
    "# Plot histogram of sentiment scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(df['sentiment'], bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Sentiment Polarity Distribution')\n",
    "plt.xlabel('Sentiment Polarity')\n",
    "plt.ylabel('Number of Reviews')\n",
    "plt.show()\n",
    "\n",
    "# Combine all reviews text into one string for WordCloud\n",
    "text_combined = \" \".join(reviews_clean)\n",
    "\n",
    "# Generate WordCloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=100).generate(text_combined)\n",
    "\n",
    "# Plot WordCloud\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Reviews')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
